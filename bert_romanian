import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizerFast, BertForSequenceClassification
from torch.optim import AdamW
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from tqdm.auto import tqdm
import argparse
import os

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# =====================
# Dataset Personalizat
# =====================
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        encoding = self.tokenizer(
            text,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# =====================
# FuncÈ›ii de training/evaluare
# =====================
def train_epoch(model, data_loader, optimizer):
    model.train()
    losses = []
    for batch in tqdm(data_loader, desc="Training"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        losses.append(loss.item())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    return np.mean(losses)

def eval_model(model, data_loader):
    model.eval()
    preds, probs, labels_all = [], [], []
    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits

            probs_batch = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()
            preds_batch = torch.argmax(logits, dim=1).cpu().numpy()

            probs.extend(probs_batch)
            preds.extend(preds_batch)
            labels_all.extend(labels.cpu().numpy())

    acc = accuracy_score(labels_all, preds)
    f1 = f1_score(labels_all, preds)
    try:
        roc_auc = roc_auc_score(labels_all, probs)
    except ValueError:
        roc_auc = float('nan')
    return acc, f1, roc_auc

# =====================
# Cross-validation
# =====================
def run_cross_validation(texts, labels, epochs=3, batch_size=16, max_len=128, n_splits=5):
    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    fold_results, train_results = [], []

    for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):
        print(f'\n=== Fold {fold + 1}/{n_splits} ===')
        train_texts = [texts[i] for i in train_idx]
        train_labels = [labels[i] for i in train_idx]
        val_texts = [texts[i] for i in val_idx]
        val_labels = [labels[i] for i in val_idx]

        train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len)
        val_dataset = TextDataset(val_texts, val_labels, tokenizer, max_len)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)

        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)
        optimizer = AdamW(model.parameters(), lr=2e-5)

        for epoch in range(epochs):
            train_loss = train_epoch(model, train_loader, optimizer)
            print(f'Epoch {epoch + 1}/{epochs} - Train loss: {train_loss:.4f}')

        train_acc, train_f1, train_auc = eval_model(model, train_loader)
        val_acc, val_f1, val_auc = eval_model(model, val_loader)

        print(f'âœ… Train - Acc: {train_acc:.4f}, F1: {train_f1:.4f}, AUC: {train_auc:.4f}')
        print(f'âœ… Valid - Acc: {val_acc:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}')

        train_results.append((train_acc, train_f1, train_auc))
        fold_results.append((val_acc, val_f1, val_auc))

    val_accs, val_f1s, val_rocs = zip(*fold_results)
    train_accs, train_f1s, train_rocs = zip(*train_results)
    print('\n=== Cross-Validation Summary ===')
    print(f'ðŸ“Š Validation Accuracy: {np.mean(val_accs):.4f} Â± {np.std(val_accs):.4f}')
    print(f'ðŸ“Š Validation F1 Score: {np.mean(val_f1s):.4f} Â± {np.std(val_f1s):.4f}')
    print(f'ðŸ“Š Validation ROC AUC: {np.mean(val_rocs):.4f} Â± {np.std(val_rocs):.4f}')
    print(f'ðŸ“ˆ Train Accuracy: {np.mean(train_accs):.4f} Â± {np.std(train_accs):.4f}')
    print(f'ðŸ“ˆ Train F1 Score: {np.mean(train_f1s):.4f} Â± {np.std(train_f1s):.4f}')
    print(f'ðŸ“ˆ Train ROC AUC: {np.mean(train_rocs):.4f} Â± {np.std(train_rocs):.4f}')

# =====================
# Main
# =====================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="BERT Text Classification")
    parser.add_argument('--data_path', type=str, default='data/ro_dataset_clasificare_human_vs_ai.csv', help='Calea cÄƒtre fiÈ™ierul CSV')
    args = parser.parse_args()

    if not os.path.exists(args.data_path):
        raise FileNotFoundError(f"FiÈ™ierul {args.data_path} nu a fost gÄƒsit!")

    df = pd.read_csv(args.data_path)
    texts = df['text'].tolist()
    labels = df['label'].tolist()
    run_cross_validation(texts, labels, epochs=3, batch_size=16, max_len=128, n_splits=5)

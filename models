"""
Model Comparison Script
-----------------------
This script compares multiple text classification models for distinguishing
human-written and AI-generated Romanian descriptions.

Dataset: ro_dataset_clasificare_human_vs_ai.csv
Models: Logistic Regression, Naive Bayes, Linear SVM, XGBoost
Metrics: Accuracy, F1-score, ROC AUC (Train and Cross-validation)

Outputs:
1. Printed table with metrics
2. Saved CSV file with metrics
3. Barplots comparing Train vs CV scores
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_predict, StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from xgboost import XGBClassifier

# ===== CONFIGURATION =====
DATASET_PATH = "ro_dataset_clasificare_human_vs_ai.csv"
OUTPUT_RESULTS = "model_comparison_results.csv"
MAX_FEATURES = 5000
N_SPLITS = 5
RANDOM_STATE = 42

# ===== LOAD DATA =====
df = pd.read_csv(DATASET_PATH)
X_text = df['text']
y = df['label']

# ===== VECTORIZE TEXT =====
vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)
X = vectorizer.fit_transform(X_text)

# ===== DEFINE MODELS =====
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Naive Bayes": MultinomialNB(),
    "Linear SVM": LinearSVC(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', verbosity=0)
}

# ===== CROSS-VALIDATION CONFIG =====
cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)

# ===== EVALUATION =====
results = []
for name, model in models.items():
    print(f"\nEvaluating: {name}")

    # Predictions for CV
    y_pred_cv = cross_val_predict(model, X, y, cv=cv, method='predict')

    if hasattr(model, "predict_proba"):
        y_scores_cv = cross_val_predict(model, X, y, cv=cv, method='predict_proba')[:, 1]
    else:
        y_scores_cv = cross_val_predict(model, X, y, cv=cv, method='decision_function')

    # Train on all data
    model.fit(X, y)
    y_pred_train = model.predict(X)
    if hasattr(model, "predict_proba"):
        y_scores_train = model.predict_proba(X)[:, 1]
    else:
        y_scores_train = model.decision_function(X)

    results.append({
        "Model": name,
        "Train Accuracy": accuracy_score(y, y_pred_train),
        "CV Accuracy": accuracy_score(y, y_pred_cv),
        "Train F1": f1_score(y, y_pred_train),
        "CV F1": f1_score(y, y_pred_cv),
        "Train ROC AUC": roc_auc_score(y, y_scores_train),
        "CV ROC AUC": roc_auc_score(y, y_scores_cv),
    })

# ===== SAVE & DISPLAY RESULTS =====
results_df = pd.DataFrame(results)
results_df.to_csv(OUTPUT_RESULTS, index=False)
print("\nðŸ“Š Model Comparison Results:")
print(results_df)

# ===== VISUALIZATION =====
metrics_to_plot = ["Accuracy", "F1", "ROC AUC"]
for metric in metrics_to_plot:
    plt.figure(figsize=(8, 5))
    sns.barplot(
        data=pd.melt(results_df, id_vars=["Model"], value_vars=[f"Train {metric}", f"CV {metric}"]),
        x="Model", y="value", hue="variable", palette="Set2"
    )
    plt.title(f"Train vs CV - {metric}")
    plt.ylabel(metric)
    plt.xticks(rotation=45)
    plt.ylim(0, 1)
    plt.tight_layout()
    plt.show()

print(f"\nâœ… Results saved to {OUTPUT_RESULTS}")
